# ENV: Sets the application environment.
# Used to differentiate between development and production behaviors.
# Can be 'dev' for development, or other values as needed by your application logic.
ENV=dev

# LLM Configuration (for the main response generator)
#
# LLM_PROVIDER: Specifies the large language model (LLM) provider to use for generating responses.
# Supported values: 'ollama', 'openai', 'anthropic', 'cohere', 'gemini'.
LLM_PROVIDER=ollama
# LLM_MODEL_NAME: The specific model name from the chosen LLM provider.
# Example for Ollama: 'mistral'; for OpenAI: 'gpt-4o'; for Gemini: 'gemini-pro'.
LLM_MODEL_NAME=mistral
# LLM_API_KEY: Your API key for the chosen LLM provider.
# If using Ollama locally without an API key, this can be an empty string or '1234' as a placeholder.
LLM_API_KEY=1234
# LLM_TEMPERATURE: Controls the randomness of the LLM's output.
# A value of 0 makes the output more deterministic and factual. Higher values lead to more creative responses.
LLM_TEMPERATURE=0

# TOOL_EVALUATOR_LLM Configuration (for deciding which tool to use, e.g., RAG or direct response)
#
# TOOL_EVALUATOR_LLM_PROVIDER: LLM provider for the tool evaluator. Defaults to LLM_PROVIDER if not set.
TOOL_EVALUATOR_LLM_PROVIDER=ollama
# TOOL_EVALUATOR_LLM_MODEL_NAME: Model name for the tool evaluator. Defaults to LLM_MODEL_NAME if not set.
TOOL_EVALUATOR_LLM_MODEL_NAME=mistral
# TOOL_EVALUATOR_LLM_API_KEY: API key for the tool evaluator LLM. Defaults to LLM_API_KEY if not set.
TOOL_EVALUATOR_LLM_API_KEY=1234
# TOOL_EVALUATOR_LLM_TEMPERATURE: Temperature for the tool evaluator LLM. Defaults to LLM_TEMPERATURE if not set.
TOOL_EVALUATOR_LLM_TEMPERATURE=0

# TEST LLM Configuration (for simulated user in integration tests)
#
# TEST_LLM_PROVIDER: Specifies the LLM provider to simulate the user side of the conversation.
# Supported values: 'ollama', 'openai', 'anthropic', 'cohere', 'gemini'.
TEST_LLM_PROVIDER=ollama
# TEST_LLM_MODEL_NAME: The specific model name from the chosen LLM provider for test purposes.
# Example for Ollama: 'mistral'; for OpenAI: 'gpt-4o-mini'.
TEST_LLM_MODEL_NAME=mistral
# TEST_LLM_API_KEY: API key for the test LLM provider.
# If using Ollama locally without an API key, this can be an empty string or '1234' as a placeholder.
TEST_LLM_API_KEY=1234
# TEST_LLM_STOP: Optional stop sequence for the test LLM. Can be left empty if not needed.
TEST_LLM_STOP=
# TEST_LLM_TEMPERATURE: Controls randomness for the test LLM outputs.
# Use a slightly higher temperature (e.g., 0.7) to make simulated user responses more varied and human-like.
TEST_LLM_TEMPERATURE=0.7

# SUMMARIZE LLM Configuration (for the summarization step)
#
# SUMMARIZE_LLM_PROVIDER: LLM provider specifically for summarization. If unset,
# falls back to LLM_PROVIDER (see code: LLMProvider(os.getenv("SUMMARIZE_LLM_PROVIDER") or LLM_PROVIDER)).
# Supported values: 'ollama', 'openai', 'anthropic', 'cohere', 'gemini'.
SUMMARIZE_LLM_PROVIDER=ollama
# SUMMARIZE_LLM_MODEL_NAME: Model name used by the summarizer. If unset, falls back to LLM_MODEL_NAME.
# Example for Ollama: 'mistral'; for OpenAI: 'gpt-4o-mini'.
SUMMARIZE_LLM_MODEL_NAME=mistral
# SUMMARIZE_LLM_API_KEY: API key for the summarizer model. If unset or empty,
# falls back to LLM_API_KEY (SecretStr wrapping per your code).
SUMMARIZE_LLM_API_KEY=1234
# SUMMARIZE_LLM_TEMPERATURE: Temperature for the summarizer model. If unset,
# falls back to LLM_TEMPERATURE. Use 0 for deterministic summaries.
SUMMARIZE_LLM_TEMPERATURE=0
# SUMMARIZE_LLM_STOP: Optional comma-separated list of stop sequences for the summarizer.
# Leave empty to disable. Example: "</s>,###"
# Your code splits on commas into a Python list when non-empty.
SUMMARIZE_LLM_STOP=

# TEXT_EMBEDDING Configuration (for vector database embeddings)
#
# TEXT_EMBEDDING_PROVIDER: LLM provider for generating text embeddings. Defaults to LLM_PROVIDER.
# Supported values: 'ollama', 'openai', 'cohere', 'gemini'.
TEXT_EMBEDDING_PROVIDER=ollama
# TEXT_EMBEDDING_MODEL_NAME: Model name for text embeddings.
# Example for Gemini: 'models/text-embedding-004'.
TEXT_EMBEDDING_MODEL_NAME=mistral
# TEXT_EMBEDDING_API_KEY: API key for the embedding provider. Defaults to LLM_API_KEY.
TEXT_EMBEDDING_API_KEY=1234

# General File Paths
#
# DATA_DIR: Directory for persistent data (e.g., uploaded documents for RAG).
# Use 'data' for local runs, or '/app/data' when running inside Docker containers.
DATA_DIR=data
# PROMPTS_DIR: Directory where prompt templates are stored.
# Use 'prompts' for local runs, or '/app/prompts' when running inside Docker containers.
PROMPTS_DIR=prompts

# Frontend API URL
#
# API_URL: The base URL of the FastAPI backend. The frontend will use this to communicate.
API_URL=http://localhost:8000

# Database Configuration (for chat history and checkpointing)
#
# POSTGRES_URI: Connection string for your PostgreSQL database.
# If running with Docker Compose, this will typically point to the 'postgres' service within the Docker network.
POSTGRES_URI=postgresql://postgres:postgres@localhost:5432/postgres?sslmode=disable

# Vector Database Configuration (Milvus)
#
# MILVUS_URI: The URI for your Milvus vector database instance.
# If running with Docker Compose, this will typically point to the 'milvus' service.
MILVUS_URI=http://localhost:19530
# MILVUS_USERNAME: (Optional) Username for Milvus authentication.
MILVUS_USERNAME=admin
# MILVUS_PASSWORD: (Optional) Password for Milvus authentication.
MILVUS_PASSWORD=your_password
# MILVUS_COLLECTION: The name of the collection in Milvus where documents will be stored.
MILVUS_COLLECTION=your_collection_name

# Switches between `evaluate_tools` -> `generate_response` (two LLM calls)
# and `evaluate_tools` with `generate_response`. Also changes the example
# file for `evaluate_tools` prompt to `evaluate_tools_parallel.example.md`
PARALLEL_GENERATION=false
